{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the algorithm:**\n",
    "#take a big corpus with trite similes (The Daily Mail, http://cs.nyu.edu/~kcho/DMQA/, or a pulp fiction book), prepare clean sentences, traverce throug the corpus sen by sen, POS-tagging each and looking for sentences containing prepositions \"like\" and \"as\" (tag = \"IN\") and exclude \"as soon as, as well as, as usual, such as, as of yet, as much, like that, like this..\" ((alternatively, use dependency parser to accurately cut out a phrase. but it's a pain and may be an overkill)) Add these sentences to a target corpus. Cut out a simile candidate out of each sentence; optionally: replace \"likes\" and \"ases\" with a \"comparator\". \n",
    "#approach 1:\n",
    "#Use fuzzywazzy to do fuzzy matching of the simile candidates across the corpus. Find those that at least 98% similar and appear multiple times (over 10) across the corpus - those are thrite similes (or common grammatical constuctions containing 'like' or 'as' that we missed during cleaning). Build a corpus of trite similes. With a testing set, repeat all steps up to fuzzywazzy. Then, instead of fuzzy matching candidates across the testinf set, fuzzy match them with the trite similes corpus. Highlight (tag) if a match is found.  \n",
    "#approach 2: \n",
    "#Sort words in each set alphabetically. Then build an n-gram counter (may be plot a histogram) - a dictionary with an n-gram as a key and how many times is appears in the corpus as a value. In a new text, repeat all steps up to the last one and then find new n-grams in the dictionary. If the new n-grams are among the most frequently met n-grams in the corpus, these n-grams constitute trite similes. Then use them as a trite similes corpus and compare to the testing set as described in the approach 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "min_simile_freq = 5\n",
    "train_dir_name = '../raw_data/similes_train/' \n",
    "test_dir_name = '../raw_data/similes_test/' \n",
    "\n",
    "\n",
    "# from nltk.parse.stanford import StanfordDependencyParser\n",
    "# path_to_jar = '/Development/Projects/Magnifis/3rd_party/NLU/stanford-corenlp-full-2013/stanford-corenlp-3.2.0.jar'\n",
    "# path_to_models_jar ='/Development/Projects/Magnifis/3rd_party/NLU/stanford-corenlp-full-2013/stanford-corenlp-3.2.0-models.jar'\n",
    "# dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "# result = dependency_parser.raw_parse('I shot an elephant in my sleep')\n",
    "# dep = result.next()\n",
    "# list(dep.triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "\n",
    "regex_filter = r\"(as soon)|(as well)|(as if)|(as \\w+ as possible)|(as before)|\\\n",
    "(as long)|(as usual)|(as ever)|(as a result)|\\\n",
    "(such as)|(as of yet)|(as much)|(as many)|\\\n",
    "(like that)|(like this)|(like you)|(like me)|(like him)|(like us)|(like her)|\\\n",
    "(like everything else)|(like everyone else)|(anybody like)|(anyone like)\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_raw_text_data(input_dir):  \n",
    "    fList=os.listdir(input_dir)\n",
    "    # Create a list of file names with full path, eliminating subdirectory entries\n",
    "    fList1 = [os.path.join(input_dir, f) for f in fList if os.path.isfile(os.path.join(input_dir, f))] \n",
    "    \n",
    "    #max_files = 1000 #remove to get the entire corpus\n",
    "    raw_corpus = ''\n",
    "    for file in tqdm(fList1): #[0:max_files] \n",
    "        with codecs.open(file, 'r', 'latin_1') as f: \n",
    "                                        # 'utf-8') as f:\n",
    "        #with open(file, encoding=\"utf8\") as f:\n",
    "            raw_corpus += ''.join(f.read())  \n",
    "    corpus = re.sub(r\"(\\n|\\r)+\"\"|(@\\w+)+\", ' ', raw_corpus) #remove backslashes and words starting with @\n",
    "    #corpus = re.sub(r\"(as soon)+\" \"|(as well)+\" \"|(as if)+\" \"|(as quickly as possible)+\" \"|(as long)\" \"|(as usual)+\" \"|(such as)+\" \"|(as of yet)+\" \"|(as much)+\" \"|(as many)+\" \"|(like that)+\" \"|(like this)+\" \"|(like you)+\" \"|(like me)+\" \"|(like him)+\" \"|(like us)+\" \"|(like her)+\" \"|(anybody like)+\" \"|(anyone like)+\", \"\", corpus)\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(corpus, regex_filter, do_tokenize_words=True):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences \n",
    "    if regex_filter:\n",
    "        raw_sents = [sent for sent in raw_sents if not re.search(regex_filter, sent)]\n",
    "    if do_tokenize_words:\n",
    "        result = [nltk.word_tokenize(sent) for sent in raw_sents]\n",
    "    else: \n",
    "        result = raw_sents\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_simile_candidates(sentences):\n",
    "    comparisons = []\n",
    "    for sent in sentences:\n",
    "        if not 'like' in sent and not 'as' in sent: \n",
    "            continue \n",
    "        # exlude a single 'as', leaving in only '...as ... as...'\n",
    "        if not 'like' in sent and len([word for word in sent if word=='as']) == 1: \n",
    "            continue\n",
    "        pos_tagged = nltk.pos_tag(sent)\n",
    "        for pair in pos_tagged:\n",
    "            if pair[1] == 'IN' and (pair[0] == 'like' or pair[0] == 'as'):\n",
    "                comparisons.append(pos_tagged)\n",
    "    return comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_candidates(all_candidates):\n",
    "    similes_candidates = []\n",
    "    punkt = set(['.',',','-',':',';','!','?', '\"', '\\'', ')', '(', '%', '#', '[', ']', '@'])\n",
    "    key_pos_tags = set(['NN', 'NNS', 'NNP']) #, 'VB', 'VBN', 'VBD', 'VBG']) # noun or verb\n",
    "    for tagged_sent in all_candidates:\n",
    "        start_index = -1\n",
    "        words_after = -1\n",
    "        sent = [pair[0] for pair in tagged_sent]\n",
    "        pos_tags = [pair[1] for pair in tagged_sent]\n",
    "        if 'like' in sent:\n",
    "            start_index = sent.index('like')\n",
    "            #two_words_before_like = max(0, index_of_like - 4)\n",
    "            words_after = min(len(sent), start_index + 6)\n",
    "        elif 'as' in sent:\n",
    "            start_index = sent.index('as')\n",
    "            words_after = min(len(sent), start_index + 8)\n",
    "\n",
    "        if start_index >= 0 and words_after > 0:\n",
    "            index_of_punkt = 0\n",
    "            for i in range(start_index, words_after): \n",
    "                if sent[i] in punkt: \n",
    "                    index_of_punkt = i\n",
    "                    break \n",
    "\n",
    "            if index_of_punkt > start_index: \n",
    "                words_after = min(words_after, index_of_punkt)\n",
    "            if not(not key_pos_tags.intersection(set(pos_tags[start_index:words_after]))): # make sure at least one key pos tag is present\n",
    "                similes_candidates.append(sent[start_index:words_after])\n",
    "    return similes_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(['a', 'an', 'and', 'or', 'the', \\\n",
    "                  'his', 'her', 'my', 'your', 'their', 'our', \\\n",
    "                  'i', 'you', 'he', 'she', 'it', 'they', 'who', 'that', 'whose', \\\n",
    "                  'is', 'are', 'was', 'will', 'would', \\\n",
    "                  '.',',','-',':',';','!','?', '\"', '\\'', ')', '(', '%', '#', '[', ']', '@'])\n",
    "\n",
    "def preprocess_words(wordlist): \n",
    "    wordset = set([])\n",
    "    for word in wordlist: \n",
    "        word = word.lower()\n",
    "        if word not in stop_words and len(word) > 1: \n",
    "            if word != 'as':\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "            if word == 'like' or word == 'as': \n",
    "                word = '$cmpr'\n",
    "            wordset.add(word)\n",
    "    return wordset \n",
    "        \n",
    "''' Precomputes a corpus (phrase search index) for a given list of phrases\n",
    "    Optimization: create a data structure to speed up fuzzy matching as follows: \n",
    "    {'word' : [i, j, k, ...]}, where i, j, k are the row indices of all phrases containing 'word'. \n",
    "    For each new search query, we prefetch the relevant rows based in the words in that query, \n",
    "    prior to fuzzy matching. \n",
    "'''\n",
    "def init_corpus_2match(wordlists): \n",
    "    lookup = {}\n",
    "    all_wordsets = []\n",
    "    for words in wordlists: # for each phrase (word list)\n",
    "        if not words:\n",
    "            continue\n",
    "        wordset = preprocess_words(words)\n",
    "        if not(not wordset):\n",
    "            i_row = len(all_wordsets)   \n",
    "            all_wordsets.append(wordset)\n",
    "            \n",
    "            # update loookup index (dictionary of word to corpus row id)\n",
    "            for word in wordset: \n",
    "                if word not in lookup: \n",
    "                    lookup[word] = [i_row]\n",
    "                else: \n",
    "                    lookup[word].append(i_row)\n",
    "    return (all_wordsets, lookup) \n",
    "\n",
    "\n",
    "''' Returns a list of matches for 'phrase' in 'wordsets' with 'min_similarity' \n",
    "'''\n",
    "def fuzzy_match(words_in, search_index, min_similarity): \n",
    "    # init \n",
    "    phraset = preprocess_words(words_in)\n",
    "    relevant_corpus_rows = search_index\n",
    "    \n",
    "    # prepare relevant subset of search index\n",
    "    # the data could be in 2 different representations\n",
    "    if isinstance(search_index, tuple): \n",
    "        corpus = search_index[0]\n",
    "        lookup = search_index[1]\n",
    "\n",
    "        # prefetch relevant corpus rows \n",
    "        relevant_corpus_row_ids = set([])\n",
    "        for word in phraset: \n",
    "            if word not in lookup:\n",
    "                continue\n",
    "            row_ids = lookup[word]\n",
    "            for i in row_ids:\n",
    "                relevant_corpus_row_ids.add(i)    \n",
    "        relevant_corpus_rows = [corpus[i] for i in relevant_corpus_row_ids]  \n",
    "        \n",
    "    # actually search\n",
    "    nb_input = len(phraset)\n",
    "    matches = []\n",
    "    for wordset in relevant_corpus_rows: \n",
    "        intersect = phraset.intersection(wordset)\n",
    "        n = len(intersect)\n",
    "        if n/min(nb_input, len(wordset)) >= min_similarity and not(n < 2 and next(iter(intersect))=='$cmpr'): \n",
    "            #print(wordset)\n",
    "            matches.append(wordset)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def train_similes_corpus(candidates):\n",
    "    corpus_2match = init_corpus_2match(candidates)\n",
    "    covered = set([])\n",
    "    count_dict = {}\n",
    "    for cand in candidates:\n",
    "        if not cand: \n",
    "            continue\n",
    "        phrase = ' '.join(cand)\n",
    "        if phrase in covered:\n",
    "            continue\n",
    "        covered.add(phrase)\n",
    "        result = fuzzy_match(cand, corpus_2match, 0.75)\n",
    "        #print(\"result is {}\".format(result))\n",
    "        if result:\n",
    "            count_dict[phrase] = len(result)\n",
    "    \n",
    "    sorted_counts = sorted(count_dict.items(), key=operator.itemgetter(1))\n",
    "    sorted_counts.reverse()\n",
    "    return count_dict, sorted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def aggregate_similes_candidates(input_dir):  \n",
    "    fList=os.listdir(input_dir)\n",
    "    # Create a list of file names with full path, eliminating subdirectory entries\n",
    "    fList1 = [os.path.join(input_dir, f) for f in fList if os.path.isfile(os.path.join(input_dir, f))] \n",
    "    \n",
    "    #max_files = 1000 #remove to get the entire corpus\n",
    "    all_candidates = []\n",
    "    for i in tqdm(range(len(fList1))): #[0:max_files] \n",
    "        file = fList1[i]\n",
    "        with codecs.open(file, 'r', 'latin_1') as f: \n",
    "                                        # 'utf-8') as f:\n",
    "        #with open(file, encoding=\"utf8\") as f:\n",
    "            raw_text = ''.join(f.read()) \n",
    "            text = re.sub(r\"(\\n|\\r)+\"\"|(@\\w+)+\", ' ', raw_text) #remove backslashes and words starting with @\n",
    "            sentences = tokenize_text(text, regex_filter)\n",
    "            similes_candidates = extract_simile_candidates(sentences)\n",
    "            similes_candidates = filter_candidates(similes_candidates)\n",
    "            all_candidates.extend(similes_candidates)\n",
    "    return all_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract simile candidates from raw text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "def train(input_dir, min_simile_freq): \n",
    "    similes_candidates = aggregate_similes_candidates(input_dir)\n",
    "    count_dict, sorted_counts = train_similes_corpus(similes_candidates)\n",
    "\n",
    "    # create actual corpus and save \n",
    "    top_similes_corpus = init_corpus_2match([item[0].split(' ') for item in count_dict.items() if item[1] >= min_simile_freq])\n",
    "    # save \n",
    "    joblib.dump(top_similes_corpus, \"top_similes_corpus.v2.pkl\")\n",
    "    return similes_candidates, sorted_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5032/5032 [2:26:19<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "similes_candidates, sorted_counts = train(train_dir_name, min_simile_freq)\n",
    "sorted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similes_candidates[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_tagged_simile_sents(sentences):\n",
    "    simile_sents = []\n",
    "    for sent in sentences: \n",
    "        if not re.search(\"<rule1s>\", sent):\n",
    "            continue\n",
    "        sent = re.sub(r\"(<rule1s>)|(</rule1s>)\", \"\", sent)\n",
    "        simile_sents.append(nltk.pos_tag(nltk.word_tokenize(sent))) \n",
    "    return simile_sents\n",
    "\n",
    "\n",
    "# Test last step: (pseudo-)\"classification\" of simile_candidates\n",
    "def test(data_dir, similes_corpus, min_simile_freq): \n",
    "    raw_corpus = get_raw_text_data(data_dir)\n",
    "    sentences = tokenize_text(raw_corpus, None, do_tokenize_words=False)\n",
    "    true_simile_sents = extract_tagged_simile_sents(sentences)\n",
    "    \n",
    "    nb_true_pos = 0\n",
    "    false_pos = []\n",
    "    false_neg = []\n",
    "    for true_simile_sent in true_simile_sents:\n",
    "        # sent_words = [pair[0] for pair in tagged_sent]\n",
    "        predicted_simile = False\n",
    "        sent = [pair[0] for pair in true_simile_sent] # remove POS tags \n",
    "        nb_matches = 0\n",
    "        nb_true_pos += 1\n",
    "        simile_candidates = filter_candidates([true_simile_sent])\n",
    "        if not (not simile_candidates): \n",
    "            simile_candidate = simile_candidates[0]\n",
    "            matches = fuzzy_match(simile_candidate, similes_corpus, 0.75)\n",
    "            nb_matches = len(matches)\n",
    "        \n",
    "        if nb_matches >= min_simile_freq:\n",
    "            predicted_simile = True\n",
    "\n",
    "        if not predicted_simile:\n",
    "            false_neg.append(' '.join(sent))\n",
    "#         else \n",
    "#             print(\"'{}' is NOT a trite simile\".format(cand))\n",
    "    precision = nb_true_pos / (nb_true_pos + len(false_pos))\n",
    "    recall = nb_true_pos / (nb_true_pos + len(false_neg))\n",
    "    print(\"=== Claddification Report ===\")\n",
    "    print(\"Precision = {}\".format(precision))\n",
    "    print(\"Recall = {}\".format(recall))\n",
    "    print(\"=============================\")\n",
    "    print (\"-- False Negatives --\")\n",
    "    for neg in false_neg:\n",
    "        print(neg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 274.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Claddification Report ===\n",
      "Precision = 1.0\n",
      "Recall = 0.5294117647058824\n",
      "=============================\n",
      "-- False Negatives --\n",
      "As big as a pigeonâs egg , he wrote , and as blue as the sea , but with a flare of red at its core .\n",
      "Smokestacks fume and locomotives trundle back and forth on elevated conduits and leafless trees stand atop slag heaps like skeleton hands shoved up from the underworld .\n",
      "The breast feathers of a stuffed and mounted chickadee are impossibly soft , its beak as sharp as a needle .\n",
      "But he watches his sisterâs face , motionless except for her eyelids , and in the kitchen Frau Elena holds her flour-whitened hands in the air and cocks her head , studying Werner , and two older boys rush in and stop , sensing some change in the air , and the little radio with its four terminals and trailing aerial sits motionless on the floor between them all like a miracle .\n",
      "Mysterious Mr. Fogg lives his life like a machine .\n",
      "Then a yellow moon rose like a flower growing and lit the empty steppe deep into the shadowy distance .\n",
      "He thought nobody was looking , but I saw his hoof as plain as day.â He stared at Yakov with the bloody eye .\n",
      "I say we ought to call our menfolk together , armed with guns , knives , pitchforks , clubsâanything that will kill a Jewâand when the church bells begin to ring we move on the Zhidy quarter , which you can tell by the stink , routing them out of wherever theyâre hidingâin attics , cellars , or ratholesâbashing in their brains , stabbing their herring-filled guts , shooting off their snotty noses , no exception made for young or old , because if you spare any they breed like rats and then the jobâs to do all over again .\n",
      "He sought , he continued to say to himself , opportunities , though in seeking them he sometimes felt like a spy behind enemy lines .\n",
      "It used to be a perfectly ordinary day but now it sticks up on the calendar like a rusty nail . )\n",
      "Yet the museum always felt like a holiday ; and once we were inside with the glad roar of tourists all around us , I felt strangely insulated from whatever else the day might hold in store .\n",
      "As I hung behind my mother in the admissions line , I put my head back and stared fixedly into the cavernous ceiling dome two stories above : if I stared hard enough , sometimes I could make myself feel like I was floating around up there like a feather , a trick from early childhood that was fading as I got older .\n",
      "She was accompanied by a funny old white-haired character who I guessed from his sharpness of face was related to her , her grandfather maybe : houndstooth coat , long narrow lace-up shoes as shiny as glass .\n",
      "My ears rang , and so did my body , an intensely disturbing sensation : bones , brain , heart all thrumming like a struck bell .\n",
      "My jaw hurt ; my face and knees were cut ; my mouth was like sandpaper .\n",
      "He was trying to look up at me , but his head dangled heavily on his neck and his chin lolled on his chest so that he was forced to peer from under his brow at me like a vulture .\n",
      "His lower half lay twisted on the ground like a pile of dirty clothes .\n",
      "âIâll wait till they come.â âYouâre so kind.â His hand ( cold , dry as powder ) tightening on mine .\n",
      "There was a green lizard that lived in the palm tree , green like a candy drop , I loved to watch for himâ¦ flashing on the windowsillâ¦ fairy lights in the gardenâ¦ du pays saintâ¦ twenty minutes to walk it but it seemed like milesâ¦â He faded for a minute ; I could feel his intelligence drifting away from me , spinning out of sight like a leaf on a brook .\n",
      "Then he sank down into himself , flat and collapsed-looking like all the air was out of him , thirty seconds , forty , like a heap of old clothes but thenâso harshly I flinchedâhis chest swelled on a bellows-like rasp , and he coughed a percussive gout of blood that spewed all over me .\n",
      "Partway , my bag caught on something , and for a moment I thought I might have to slip free of it , painting or no painting , like a lizard shedding its tail , but when I gave it one last pull it finally broke free with a shower of crumbled plaster .\n",
      "I passed a cavernous dark room with a long workshop table where mismatched scraps of cloth were laid out like pieces of a jigsaw puzzle .\n",
      "âWe had to evacuate.â Before I had time to register this , a gigantic cop swooped down on me like a thunderclap : a thickheaded bulldoggish guy with pumped-up arms like a weightlifterâs .\n",
      "The Bible said guests should be treated like royalty lest a host entertain angels unaware .\n"
     ]
    }
   ],
   "source": [
    "similes_corpus = joblib.load(\"top_similes_corpus.v2.pkl\")\n",
    "test(test_dir_name, similes_corpus, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 218.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Misc unit tests \n",
    "raw_corpus = get_raw_text_data(test_dir_name)\n",
    "sentences = tokenize_text(raw_corpus, None, do_tokenize_words=False)\n",
    "true_simile_sents = extract_tagged_simile_sents(sentences)\n",
    "simile_candidates = filter_candidates(true_simile_sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['as', 'a', 'pigeonâ\\x80\\x99s', 'egg'],\n",
       " ['like', 'skeleton', 'hands', 'shoved', 'up', 'from']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simile_candidates[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Backup code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import fuzzywuzzy\n",
    "# from fuzzywuzzy import fuzz\n",
    "# from fuzzywuzzy import process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choices = []\n",
    "# for each in similes_candidates:\n",
    "#     choices.append(\" \".join(each))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count_dict = {}\n",
    "\n",
    "# for string in set(choices):\n",
    "#     result = process.extract(string, choices, limit=1000) #default limit = 5\n",
    "#     num_matches = 0\n",
    "#     for each in result:\n",
    "#         if each[1] > 98:\n",
    "#             num_matches +=1\n",
    "#     count_dict[string] = num_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write \n",
    "# from sklearn.externals import joblib\n",
    "# joblib.dump(count_dict, \"count_dict_output.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count_dict = sorted(count_dict.items(), key=operator.itemgetter(1))\n",
    "# count_dict.reverse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read \n",
    "#count_dict_fromfile = joblib.load(\"count_dict_output.pkl\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
