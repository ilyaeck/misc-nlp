{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the algorithm:**\n",
    "#take a big corpus with trite similes (The Daily Mail, http://cs.nyu.edu/~kcho/DMQA/, or a pulp fiction book), prepare clean sentences, traverce throug the corpus sen by sen, POS-tagging each and looking for sentences containing prepositions \"like\" and \"as\" (tag = \"IN\") and exclude \"as soon as, as well as, as usual, such as, as of yet, as much, like that, like this..\" ((alternatively, use dependency parser to accurately cut out a phrase. but it's a pain and may be an overkill)) Add these sentences to a target corpus. Cut out a simile candidate out of each sentence; optionally: replace \"likes\" and \"ases\" with a \"comparator\". \n",
    "#approach 1:\n",
    "#Use fuzzywazzy to do fuzzy matching of the simile candidates across the corpus. Find those that at least 98% similar and appear multiple times (over 10) across the corpus - those are thrite similes (or common grammatical constuctions containing 'like' or 'as' that we missed during cleaning). Build a corpus of trite similes. With a testing set, repeat all steps up to fuzzywazzy. Then, instead of fuzzy matching candidates across the testinf set, fuzzy match them with the trite similes corpus. Highlight (tag) if a match is found.  \n",
    "#approach 2: \n",
    "#Sort words in each set alphabetically. Then build an n-gram counter (may be plot a histogram) - a dictionary with an n-gram as a key and how many times is appears in the corpus as a value. In a new text, repeat all steps up to the last one and then find new n-grams in the dictionary. If the new n-grams are among the most frequently met n-grams in the corpus, these n-grams constitute trite similes. Then use them as a trite similes corpus and compare to the testing set as described in the approach 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "min_simile_freq = 5\n",
    "train_dir_name = './data/similes_train_tmp/' \n",
    "test_dir_name = './data/similes_test/' \n",
    "\n",
    "\n",
    "# from nltk.parse.stanford import StanfordDependencyParser\n",
    "# path_to_jar = '/Development/Projects/Magnifis/3rd_party/NLU/stanford-corenlp-full-2013/stanford-corenlp-3.2.0.jar'\n",
    "# path_to_models_jar ='/Development/Projects/Magnifis/3rd_party/NLU/stanford-corenlp-full-2013/stanford-corenlp-3.2.0-models.jar'\n",
    "# dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "# result = dependency_parser.raw_parse('I shot an elephant in my sleep')\n",
    "# dep = result.next()\n",
    "# list(dep.triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "\n",
    "regex_filter = r\"(as soon)|(as well)|(as if)|(as \\w+ as possible)|(as before)|\\\n",
    "(as long)|(as usual)|(as ever)|(as a result)|\\\n",
    "(such as)|(as of yet)|(as much)|(as many)|\\\n",
    "(like that)|(like this)|(like you)|(like me)|(like him)|(like us)|(like her)|\\\n",
    "(like everything else)|(like everyone else)|(anybody like)|(anyone like)\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_raw_text_data(input_dir):  \n",
    "    fList=os.listdir(input_dir)\n",
    "    # Create a list of file names with full path, eliminating subdirectory entries\n",
    "    fList1 = [os.path.join(input_dir, f) for f in fList if os.path.isfile(os.path.join(input_dir, f))] \n",
    "    \n",
    "    #max_files = 1000 #remove to get the entire corpus\n",
    "    raw_corpus = ''\n",
    "    for file in tqdm(fList1): #[0:max_files] \n",
    "        with codecs.open(file, 'r', 'latin_1') as f: \n",
    "                                        # 'utf-8') as f:\n",
    "        #with open(file, encoding=\"utf8\") as f:\n",
    "            raw_corpus += ''.join(f.read())  \n",
    "    corpus = re.sub(r\"(\\n|\\r)+\"\"|(@\\w+)+\", ' ', raw_corpus) #remove backslashes and words starting with @\n",
    "    #corpus = re.sub(r\"(as soon)+\" \"|(as well)+\" \"|(as if)+\" \"|(as quickly as possible)+\" \"|(as long)\" \"|(as usual)+\" \"|(such as)+\" \"|(as of yet)+\" \"|(as much)+\" \"|(as many)+\" \"|(like that)+\" \"|(like this)+\" \"|(like you)+\" \"|(like me)+\" \"|(like him)+\" \"|(like us)+\" \"|(like her)+\" \"|(anybody like)+\" \"|(anyone like)+\", \"\", corpus)\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(corpus, regex_filter, do_tokenize_words=True):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences \n",
    "    if regex_filter:\n",
    "        raw_sents = [sent for sent in raw_sents if not re.search(regex_filter, sent)]\n",
    "    if do_tokenize_words:\n",
    "        result = [nltk.word_tokenize(sent) for sent in raw_sents]\n",
    "    else: \n",
    "        result = raw_sents\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_simile_candidates(sentences):\n",
    "    comparisons = []\n",
    "    for i_sent, sent in enumerate(sentences):\n",
    "        if not 'like' in sent and not 'as' in sent: \n",
    "            continue \n",
    "        # exlude a single 'as', leaving in only '...as ... as...'\n",
    "        if not 'like' in sent and len([word for word in sent if word=='as']) == 1: \n",
    "            continue\n",
    "        pos_tagged = nltk.pos_tag(sent)\n",
    "        for pair in pos_tagged:\n",
    "            if pair[1] == 'IN' and (pair[0] == 'like' or pair[0] == 'as'):\n",
    "                comparisons.append((i_sent, pos_tagged))\n",
    "    return comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_candidates(all_candidates):\n",
    "    similes_candidates = []\n",
    "    punkt = set(['.',',','-',':',';','!','?', '\"', '\\'', ')', '(', '%', '#', '[', ']', '@'])\n",
    "    key_pos_tags = set(['NN', 'NNS', 'NNP']) #, 'VB', 'VBN', 'VBD', 'VBG']) # noun or verb\n",
    "    for i_sent, tagged_sent in all_candidates:\n",
    "        start_index = -1\n",
    "        words_after = -1\n",
    "        sent = [pair[0] for pair in tagged_sent]\n",
    "        pos_tags = [pair[1] for pair in tagged_sent]\n",
    "        if 'like' in sent:\n",
    "            start_index = sent.index('like')\n",
    "            #two_words_before_like = max(0, index_of_like - 4)\n",
    "            words_after = min(len(sent), start_index + 6)\n",
    "        elif 'as' in sent:\n",
    "            start_index = sent.index('as')\n",
    "            words_after = min(len(sent), start_index + 8)\n",
    "\n",
    "        if start_index >= 0 and words_after > 0:\n",
    "            index_of_punkt = 0\n",
    "            for i in range(start_index, words_after): \n",
    "                if sent[i] in punkt: \n",
    "                    index_of_punkt = i\n",
    "                    break \n",
    "\n",
    "            if index_of_punkt > start_index: \n",
    "                words_after = min(words_after, index_of_punkt)\n",
    "            if not(not key_pos_tags.intersection(set(pos_tags[start_index:words_after]))): # make sure at least one key pos tag is present\n",
    "                similes_candidates.append((i_sent, sent[start_index:words_after]))\n",
    "    return similes_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(['a', 'an', 'and', 'or', 'the', \\\n",
    "                  'his', 'her', 'my', 'your', 'their', 'our', \\\n",
    "                  'i', 'you', 'he', 'she', 'it', 'they', 'who', 'that', 'whose', \\\n",
    "                  'is', 'are', 'was', 'will', 'would', \\\n",
    "                  '.',',','-',':',';','!','?', '\"', '\\'', ')', '(', '%', '#', '[', ']', '@'])\n",
    "\n",
    "def preprocess_words(wordlist): \n",
    "    wordset = set([])\n",
    "    for word in wordlist: \n",
    "        word = word.lower()\n",
    "        if word not in stop_words and len(word) > 1: \n",
    "            if word != 'as':\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "            if word == 'like' or word == 'as': \n",
    "                word = '$cmpr'\n",
    "            wordset.add(word)\n",
    "    return wordset \n",
    "        \n",
    "\n",
    "def init_corpus_2match(wordlists): \n",
    "    lookup = {}\n",
    "    all_wordsets = []\n",
    "    for words in wordlists: \n",
    "        if not words:\n",
    "            continue\n",
    "        wordset = preprocess_words(words)\n",
    "        if not(not wordset):\n",
    "            i_row = len(all_wordsets)\n",
    "            all_wordsets.append(wordset)\n",
    "            \n",
    "            # update loookup index (dictionary of word to corpus row id)\n",
    "            for word in wordset: \n",
    "                if word not in lookup: \n",
    "                    lookup[word] = [i_row]\n",
    "                else: \n",
    "                    lookup[word].append(i_row)\n",
    "    return (all_wordsets, lookup) \n",
    "\n",
    "\n",
    "''' Returns a list of matches for 'phrase' in 'wordsets' with 'min_similarity' \n",
    "'''\n",
    "def fuzzy_match(words_in, search_index, min_similarity): \n",
    "    phraset = preprocess_words(words_in)\n",
    "    relevant_corpus_rows = search_index\n",
    "    \n",
    "    # the data could be in 2 different representations\n",
    "    if isinstance(search_index, tuple): \n",
    "        corpus = search_index[0]\n",
    "        lookup = search_index[1]\n",
    "\n",
    "        # prefetch relevant corpus rows \n",
    "        relevant_corpus_row_ids = set([])\n",
    "        for word in phraset: \n",
    "            if word not in lookup:\n",
    "                continue\n",
    "            row_ids = lookup[word]\n",
    "            for i in row_ids:\n",
    "                relevant_corpus_row_ids.add(i)    \n",
    "        relevant_corpus_rows = [corpus[i] for i in relevant_corpus_row_ids]  \n",
    "        \n",
    "    #print (\"Input phraseset is {}\".format(phraset))\n",
    "    nb_input = len(phraset)\n",
    "    matches = []\n",
    "    for wordset in relevant_corpus_rows: \n",
    "        intersect = phraset.intersection(wordset)\n",
    "        n = len(intersect)\n",
    "        if n/min(nb_input, len(wordset)) >= min_similarity and not(n < 2 and next(iter(intersect))=='$cmpr'): \n",
    "            #print(wordset)\n",
    "            matches.append(wordset)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def train_similes_corpus(candidates):\n",
    "    corpus_2match = init_corpus_2match(candidates)\n",
    "    covered = set([])\n",
    "    count_dict = {}\n",
    "    for cand in candidates:\n",
    "        if not cand: \n",
    "            continue\n",
    "        phrase = ' '.join(cand)\n",
    "        if phrase in covered:\n",
    "            continue\n",
    "        covered.add(phrase)\n",
    "        result = fuzzy_match(cand, corpus_2match, 0.75)\n",
    "        #print(\"result is {}\".format(result))\n",
    "        if result:\n",
    "            count_dict[phrase] = len(result)\n",
    "    \n",
    "    sorted_counts = sorted(count_dict.items(), key=operator.itemgetter(1))\n",
    "    sorted_counts.reverse()\n",
    "    return count_dict, sorted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def aggregate_similes_candidates(input_dir):  \n",
    "    fList=os.listdir(input_dir)\n",
    "    # Create a list of file names with full path, eliminating subdirectory entries\n",
    "    fList1 = [os.path.join(input_dir, f) for f in fList if os.path.isfile(os.path.join(input_dir, f))] \n",
    "    \n",
    "    #max_files = 1000 #remove to get the entire corpus\n",
    "    all_candidates = []\n",
    "    for i in tqdm(range(len(fList1))): #[0:max_files] \n",
    "        file = fList1[i]\n",
    "        with codecs.open(file, 'r', 'latin_1') as f: \n",
    "                                        # 'utf-8') as f:\n",
    "        #with open(file, encoding=\"utf8\") as f:\n",
    "            raw_text = ''.join(f.read()) \n",
    "            text = re.sub(r\"(\\n|\\r)+\"\"|(@\\w+)+\", ' ', raw_text) #remove backslashes and words starting with @\n",
    "            sentences = tokenize_text(text, regex_filter)\n",
    "            similes_candidates = extract_simile_candidates(sentences)\n",
    "            similes_candidates = filter_candidates(similes_candidates)\n",
    "            all_candidates.extend(similes_candidates)\n",
    "    return all_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract simile candidates from raw text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "def train(input_dir, min_simile_freq): \n",
    "    similes_candidates = aggregate_similes_candidates(input_dir)\n",
    "    count_dict, sorted_counts = train_similes_corpus(similes_candidates)\n",
    "\n",
    "    # create actual corpus and save \n",
    "    top_similes_corpus = init_corpus_2match([item[0].split(' ') for item in count_dict.items() if item[1] >= min_simile_freq])\n",
    "    # save \n",
    "    joblib.dump(top_similes_corpus, \"top_similes_corpus.v3.pkl\")\n",
    "    return similes_candidates, sorted_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 59/5032 [01:25<1:58:38,  1.43s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "similes_candidates, sorted_counts = train(train_dir_name, min_simile_freq)\n",
    "sorted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similes_candidates[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_tagged_simile_sents(sentences):\n",
    "    simile_sents = []\n",
    "    for sent in sentences: \n",
    "        if not re.search(\"<rule1s>\", sent):\n",
    "            continue\n",
    "        sent = re.sub(r'(<[*rule1s[ ]*>)|(<[ ]*/rule1s[ ]*>)', \"\", sent)\n",
    "        simile_sents.append(nltk.pos_tag(nltk.word_tokenize(sent))) \n",
    "    return simile_sents\n",
    "\n",
    "\n",
    "def eval(sentence_text, similes_corpus, min_simile_freq): \n",
    "    sentences = tokenize_text(sentence_text, None, do_tokenize_words=True)\n",
    "    similes_candidates = extract_simile_candidates(sentences)\n",
    "    similes_candidates = filter_candidates(similes_candidates)\n",
    "    results = []\n",
    "    for i_sent, cand in similes_candidates:\n",
    "        predicted_simile = False\n",
    "        matches = fuzzy_match(cand, similes_corpus, 0.75)\n",
    "        nb_matches = len(matches)\n",
    "        if nb_matches >= min_simile_freq:\n",
    "            predicted_simile = True\n",
    "        results.append((' '.join(sentences[i_sent]), predicted_simile))\n",
    "    return results\n",
    "        \n",
    "\n",
    "\n",
    "# Test last step: (pseudo-)\"classification\" of simile_candidates\n",
    "def test(raw_corpus, similes_corpus, min_simile_freq): \n",
    "    \n",
    "    sentences = tokenize_text(raw_corpus, None, do_tokenize_words=False)\n",
    "    true_simile_sents = extract_tagged_simile_sents(sentences)\n",
    "    \n",
    "    nb_true_pos = 0\n",
    "    false_pos = []\n",
    "    false_neg = []\n",
    "    for true_simile_sent in true_simile_sents:\n",
    "       # sent_words = [pair[0] for pair in tagged_sent]\n",
    "        predicted_simile = False\n",
    "        sent = [pair[0] for pair in true_simile_sent] # remove POS tags \n",
    "        nb_matches = 0\n",
    "        nb_true_pos += 1\n",
    "        simile_candidates = filter_candidates([true_simile_sent])\n",
    "        if not (not simile_candidates): \n",
    "            simile_candidate = simile_candidates[0]\n",
    "            matches = fuzzy_match(simile_candidate, similes_corpus, 0.75)\n",
    "            nb_matches = len(matches)\n",
    "        \n",
    "        if nb_matches >= min_simile_freq:\n",
    "            predicted_simile = True\n",
    "\n",
    "        if not predicted_simile:\n",
    "            false_neg.append(' '.join(sent))\n",
    "#         else \n",
    "#             print(\"'{}' is NOT a trite simile\".format(cand))\n",
    "    precision = nb_true_pos / (nb_true_pos + len(false_pos))\n",
    "    recall = nb_true_pos / (nb_true_pos + len(false_neg))\n",
    "    print(\"=== Claddification Report ===\")\n",
    "    print(\"Precision = {}\".format(precision))\n",
    "    print(\"Recall = {}\".format(recall))\n",
    "    print(\"=============================\")\n",
    "    print (\"-- False Negatives --\")\n",
    "    for neg in false_neg:\n",
    "        print(neg)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 161.14it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-b1d9f8e413e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msimiles_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"top_similes_corpus.v1.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_raw_text_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimiles_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_simile_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-16cd10912280>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(raw_corpus, similes_corpus, min_simile_freq)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mnb_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mnb_true_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msimile_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrue_simile_sent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0msimile_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0msimile_candidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimile_candidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-6f4b05ee7896>\u001b[0m in \u001b[0;36mfilter_candidates\u001b[0;34m(all_candidates)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpunkt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'!'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m')'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'('\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'['\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m']'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'@'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mkey_pos_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NNS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NNP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, 'VB', 'VBN', 'VBD', 'VBG']) # noun or verb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagged_sent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_candidates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mstart_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mwords_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "similes_corpus = joblib.load(\"top_similes_corpus.v1.pkl\")\n",
    "test_data = get_raw_text_data(test_dir_name)\n",
    "test(test_data, similes_corpus, min_simile_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mysterious Mr. Fogg lives his life like a machine .', True)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = \"Mysterious Mr. Fogg lives his life like a machine.\"\n",
    "eval(test_data, similes_corpus, min_simile_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Backup code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import fuzzywuzzy\n",
    "# from fuzzywuzzy import fuzz\n",
    "# from fuzzywuzzy import process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choices = []\n",
    "# for each in similes_candidates:\n",
    "#     choices.append(\" \".join(each))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count_dict = {}\n",
    "\n",
    "# for string in set(choices):\n",
    "#     result = process.extract(string, choices, limit=1000) #default limit = 5\n",
    "#     num_matches = 0\n",
    "#     for each in result:\n",
    "#         if each[1] > 98:\n",
    "#             num_matches +=1\n",
    "#     count_dict[string] = num_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write \n",
    "# from sklearn.externals import joblib\n",
    "# joblib.dump(count_dict, \"count_dict_output.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count_dict = sorted(count_dict.items(), key=operator.itemgetter(1))\n",
    "# count_dict.reverse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read \n",
    "#count_dict_fromfile = joblib.load(\"count_dict_output.pkl\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
