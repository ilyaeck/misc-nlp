{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# from nltk.parse.stanford import StanfordDependencyParser\n",
    "# path_to_jar = '/Development/Projects/Magnifis/3rd_party/NLU/stanford-corenlp-full-2013/stanford-corenlp-3.2.0.jar'\n",
    "# path_to_models_jar ='/Development/Projects/Magnifis/3rd_party/NLU/stanford-corenlp-full-2013/stanford-corenlp-3.2.0-models.jar'\n",
    "# dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "# result = dependency_parser.raw_parse('I shot an elephant in my sleep')\n",
    "# dep = result.next()\n",
    "# list(dep.triples())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "raw_corpus = ''\n",
    "dir_name = './similes_train/'   \n",
    "fList=os.listdir(dir_name)\n",
    "fList1 = [os.path.join(dir_name, f) for f in fList if os.path.isfile(os.path.join(dir_name, f))] # Create a list of file names with full path, eliminating subdirectory entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x97 in position 571: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-27ad4880d7c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#with open(file, encoding=\"utf8\") as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mraw_corpus\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/IE/.conda/envs/py3/lib/python3.5/codecs.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/IE/.conda/envs/py3/lib/python3.5/codecs.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size, chars, firstline)\u001b[0m\n\u001b[1;32m    499\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m                 \u001b[0mnewchars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecodedbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfirstline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x97 in position 571: invalid start byte"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "#max_files = 1000 #remove to get the entire corpus\n",
    "raw_corpus = ''\n",
    "for file in fList1: #[0:max_files] \n",
    "    with codecs.open(file, 'r', 'latin_1') as f: \n",
    "                                    # 'utf-8') as f:\n",
    "    #with open(file, encoding=\"utf8\") as f:\n",
    "        raw_corpus += ''.join(f.read())   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "corpus = re.sub(r\"(\\n|\\r)+\"\"|(@\\w+)+\", ' ', raw_corpus) #remove backslashes and words starting with @\n",
    "#corpus = re.sub(r\"(as soon)+\" \"|(as well)+\" \"|(as if)+\" \"|(as quickly as possible)+\" \"|(as long)\" \"|(as usual)+\" \"|(such as)+\" \"|(as of yet)+\" \"|(as much)+\" \"|(as many)+\" \"|(like that)+\" \"|(like this)+\" \"|(like you)+\" \"|(like me)+\" \"|(like him)+\" \"|(like us)+\" \"|(like her)+\" \"|(anybody like)+\" \"|(anyone like)+\", \"\", corpus)\n",
    "regex_filter = r\"(as soon)|(as well)|(as if)|(as \\w+ as possible)|(as before)|\\\n",
    "(as long)|(as usual)|(as ever)|(as a result)|\\\n",
    "(such as)|(as of yet)|(as much)|(as many)|\\\n",
    "(like that)|(like this)|(like you)|(like me)|(like him)|(like us)|(like her)|\\\n",
    "(like everything else)|(like everyone else)|(anybody like)|(anyone like)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.search(regex_filter, u'as long as') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**the algorithm:**\n",
    "#take a big corpus with trite similes (The Daily Mail, http://cs.nyu.edu/~kcho/DMQA/, or a pulp fiction book), prepare clean sentences, traverce throug the corpus sen by sen, POS-tagging each and looking for sentences containing prepositions \"like\" and \"as\" (tag = \"IN\") and exclude \"as soon as, as well as, as usual, such as, as of yet, as much, like that, like this..\" ((alternatively, use dependency parser to accurately cut out a phrase. but it's a pain and may be an overkill)) Add these sentences to a target corpus. Cut out a simile candidate out of each sentence; optionally: replace \"likes\" and \"ases\" with a \"comparator\". \n",
    "#approach 1:\n",
    "#Use fuzzywazzy to do fuzzy matching of the simile candidates across the corpus. Find those that at least 98% similar and appear multiple times (over 10) across the corpus - those are thrite similes (or common grammatical constuctions containing 'like' or 'as' that we missed during cleaning). Build a corpus of trite similes. With a testing set, repeat all steps up to fuzzywazzy. Then, instead of fuzzy matching candidates across the testinf set, fuzzy match them with the trite similes corpus. Highlight (tag) if a match is found.  \n",
    "#approach 2: \n",
    "#Sort words in each set alphabetically. Then build an n-gram counter (may be plot a histogram) - a dictionary with an n-gram as a key and how many times is appears in the corpus as a value. In a new text, repeat all steps up to the last one and then find new n-grams in the dictionary. If the new n-grams are among the most frequently met n-grams in the corpus, these n-grams constitute trite similes. Then use them as a trite similes corpus and compare to the testing set as described in the approach 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(corpus, regex_filter):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences \n",
    "    if regex_filter:\n",
    "        raw_sents = [sent for sent in raw_sents if not re.search(regex_filter, sent)]\n",
    "    return [nltk.word_tokenize(word) for word in raw_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = tokenize_text(corpus, regex_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comparisons = []\n",
    "for sent in sentences:\n",
    "    if not 'like' in sent and not 'as' in sent: \n",
    "        continue \n",
    "    # exlude a single 'as', leaving in only '...as ... as...'\n",
    "    if not 'like' in sent and len([word for word in sent if word=='as']) == 1: \n",
    "        continue\n",
    "    pos_tagged = nltk.pos_tag(sent)\n",
    "    for pair in pos_tagged:\n",
    "        if pair[1] == 'IN' and (pair[0] == 'like' or pair[0] == 'as'):\n",
    "            comparisons.append(pos_tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comparisons[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_candidates(all_candidates):\n",
    "    similes_candidates = []\n",
    "    punkt = set(['.',',','-',':',';','!','?', '\"', '\\'', ')', '(', '%', '#', '[', ']', '@'])\n",
    "    key_pos_tags = set(['NN', 'NNS', 'NNP']) #, 'VB', 'VBN', 'VBD', 'VBG']) # noun or verb\n",
    "    for tagged_sent in all_candidates:\n",
    "        start_index = -1\n",
    "        words_after = -1\n",
    "        sent = [pair[0] for pair in tagged_sent]\n",
    "        pos_tags = [pair[1] for pair in tagged_sent]\n",
    "        if 'like' in sent:\n",
    "            start_index = sent.index('like')\n",
    "            #two_words_before_like = max(0, index_of_like - 4)\n",
    "            words_after = min(len(sent), start_index + 6)\n",
    "        elif 'as' in sent:\n",
    "            start_index = sent.index('as')\n",
    "            words_after = min(len(sent), start_index + 8)\n",
    "\n",
    "        if start_index >= 0 and words_after > 0:\n",
    "            index_of_punkt = 0\n",
    "            for i in range(start_index, words_after): \n",
    "                if sent[i] in punkt: \n",
    "                    index_of_punkt = i\n",
    "                    break \n",
    "\n",
    "            if index_of_punkt > start_index: \n",
    "                words_after = min(words_after, index_of_punkt)\n",
    "            if not(not key_pos_tags.intersection(set(pos_tags[start_index:words_after]))): # make sure at least one key pos tag is present\n",
    "                similes_candidates.append(sent[start_index:words_after])\n",
    "    return similes_candidates\n",
    "\n",
    "similes_candidates = filter_candidates(comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#similes_candidates[:100]\n",
    "sent = ['like', 'it', 'to' ,'have' ,'been' ,',', 'believe' ,'me', '.']\n",
    "\n",
    "sss = nltk.pos_tag(sent)\n",
    "sss #filter_candidates([sss])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(['a', 'an', 'and', 'or', 'the', \\\n",
    "                  'his', 'her', 'my', 'your', 'their', 'our', \\\n",
    "                  'i', 'you', 'he', 'she', 'it', 'they', 'who', 'that', 'whose', \\\n",
    "                  'is', 'are', 'was', 'will', 'would', \\\n",
    "                  '.',',','-',':',';','!','?', '\"', '\\'', ')', '(', '%', '#', '[', ']', '@'])\n",
    "\n",
    "def preprocess_words(wordlist): \n",
    "    wordset = set([])\n",
    "    for word in wordlist: \n",
    "        word = word.lower()\n",
    "        if word not in stop_words and len(word) > 1: \n",
    "            if word != 'as':\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "            if word == 'like' or word == 'as': \n",
    "                word = '$cmpr'\n",
    "            wordset.add(word)\n",
    "    return wordset \n",
    "        \n",
    "\n",
    "def init_corpus_2match(wordlists): \n",
    "    wordsets = []\n",
    "    for words in wordlists: \n",
    "        if not words:\n",
    "            continue\n",
    "        wordset = preprocess_words(words)\n",
    "        if not(not wordset):\n",
    "            wordsets.append(wordset)\n",
    "    return wordsets\n",
    "\n",
    "\n",
    "''' Returns a list of matches for 'phrase' in 'wordsets' with 'min_similarity' \n",
    "'''\n",
    "def fuzzy_match(words_in, corpus, min_similarity): \n",
    "    # TODO: must be optimized!!\n",
    "    phraset = preprocess_words(words_in)\n",
    "    #print (\"Input phraseset is {}\".format(phraset))\n",
    "    nb_input = len(phraset)\n",
    "    matches = []\n",
    "    for wordset in corpus: \n",
    "        intersect = phraset.intersection(wordset)\n",
    "        n = len(intersect)\n",
    "        if n/min(nb_input, len(wordset)) >= min_similarity and not(n < 2 and next(iter(intersect))=='$cmpr'): \n",
    "            #print(wordset)\n",
    "            matches.append(wordset)\n",
    "    return matches\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidates = similes_candidates\n",
    "corpus_2match = init_corpus_2match(candidates)\n",
    "covered = set([])\n",
    "count_dict = {}\n",
    "for cand in candidates:\n",
    "    if not cand: \n",
    "        continue\n",
    "    phrase = ' '.join(cand)\n",
    "    if phrase in covered:\n",
    "        continue\n",
    "    covered.add(phrase)\n",
    "    result = fuzzy_match(cand, corpus_2match, 0.75)\n",
    "    #print(\"result is {}\".format(result))\n",
    "    if result:\n",
    "        count_dict[phrase] = len(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_counts = sorted(count_dict.items(), key=operator.itemgetter(1))\n",
    "sorted_counts.reverse()\n",
    "sorted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "min_freq = 5\n",
    "\n",
    "top_results_corpus = init_corpus_2match([item[0].split(' ') for item in count_dict.items() if item[1] >= min_freq])\n",
    "\n",
    "# save \n",
    "joblib.dump(count_dict, \"top_similes_corpus.v0.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## simile classification code ##\n",
    "\n",
    "# similes_corpus = joblib.load(\"top_similes_corpus.v0.pkl\")\n",
    "# matches = fuzzy_match(new_sent_words, similes_corpus, 0.75)\n",
    "# if len(matches) >= min_freq\n",
    "#     print(\"'{}' is a trite simile\".format(new_sent_words))\n",
    "# else \n",
    "#     print(\"'{}' is NOT a trite simile\".format(new_sent_words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Backup code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import fuzzywuzzy\n",
    "# from fuzzywuzzy import fuzz\n",
    "# from fuzzywuzzy import process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choices = []\n",
    "# for each in similes_candidates:\n",
    "#     choices.append(\" \".join(each))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count_dict = {}\n",
    "\n",
    "# for string in set(choices):\n",
    "#     result = process.extract(string, choices, limit=1000) #default limit = 5\n",
    "#     num_matches = 0\n",
    "#     for each in result:\n",
    "#         if each[1] > 98:\n",
    "#             num_matches +=1\n",
    "#     count_dict[string] = num_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write \n",
    "# from sklearn.externals import joblib\n",
    "# joblib.dump(count_dict, \"count_dict_output.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count_dict = sorted(count_dict.items(), key=operator.itemgetter(1))\n",
    "# count_dict.reverse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read \n",
    "#count_dict_fromfile = joblib.load(\"count_dict_output.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
